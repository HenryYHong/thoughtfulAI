<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>What We Owe Machines · ThoughtfulAI — CDSS 94</title>
    <meta name="description" content="Reflections on the hard problem in AI: teaching machines to handle problems without right answers. Week 1 · CDSS 94." />
    <meta name="theme-color" content="#0a0b0f" />
    <link rel="stylesheet" href="styles.css" />
  </head>
  <body>
    <a href="#main" class="skip-link">Skip to content</a>
    <div class="page-bg">
      <div class="grid-overlay"></div>
      <div class="scanline"></div>
    </div>

    <header class="site-header">
      <div class="wrap">
        <a class="logo" href="./">
          <span class="logo-bracket">[</span>ThoughtfulAI<span class="logo-bracket">]</span>
          <span class="logo-label">CDSS 94</span>
        </a>
        <nav class="nav">
          <a href="./#posts">Posts</a>
          <a href="./#about">About</a>
        </nav>
      </div>
    </header>

    <main id="main">
      <div class="wrap">
        <article class="article-layout">
          <aside class="toc">
            <p class="toc-title">On this page</p>
            <nav>
              <a href="#why-im-here">Why I'm in this class</a>
              <a href="#the-hard-problem">The hard problem</a>
              <a href="#teaching-machines">Teaching machines at all</a>
              <a href="#where-it-stands">Where the project stands</a>
              <a href="#what-we-owe">What we owe</a>
              <a href="#what-clicked">What clicked for me</a>
              <a href="#sitting-with">One thing I'm sitting with</a>
            </nav>
          </aside>

          <div>
            <a class="back-link" href="./">← All posts</a>

            <header class="article-header">
              <div class="article-meta">
                <span class="date">2026-01-26</span>
                <span class="badge badge-module">Module 1</span>
                <span class="badge badge-week">Week 1</span>
              </div>
              <h1>What We Owe Machines</h1>
              <p class="article-byline">~8 min read · Fundamentals · What We Owe Machines (Jan 26)</p>
            </header>

            <div class="article-content">
              <h2 id="why-im-here">Why I'm in this class</h2>
              <p>
                I signed up for CDSS 94 because I kept running into the same gap: I could use big models and fine-tune them for tasks, but I didn’t really understand how we go from “model that predicts the next token” to “system that does what we want in the real world.” I wanted to get into the pipeline—reward models, RLHF, evals, deployment—and see where the hard decisions actually get made. Week 1 was a good reminder that a lot of those decisions aren’t purely technical; they’re about what we optimize for and who gets to decide.
              </p>

              <p>
                The hard problem in AI isn’t making machines smarter—it’s teaching them to handle problems that don’t have right answers. That line from this week’s lecture stuck with me. We’re used to optimizing for clear objectives: accuracy, reward, “correct” behavior. But a lot of what we want from AI—fairness, nuance, knowing when to say “I don’t know”—lives in the space where there is no single right answer.
              </p>

              <div class="pullquote">
                The hard problem in AI isn’t making machines smarter—it’s teaching them to handle problems that don’t have right answers.
              </div>

              <h2 id="the-hard-problem">The hard problem</h2>
              <p>
                Optimization assumes a target. In supervised learning we have labels; in RL we have reward. But many of the behaviors we care about—being helpful without being sycophantic, being honest about uncertainty, avoiding harm across diverse contexts—don’t map cleanly onto a single scalar. The lecture framed this as the central tension: we’re building systems that must act in the world, yet we often can’t fully specify what “good” looks like in advance.
              </p>

              <div class="callout">
                <p class="callout-title">Key idea</p>
                <p>
                  We don’t just build capability; we choose what counts as success. That choice is where ethics and design meet—and where “what we owe machines” starts to look a lot like “what we owe each other when we build things that act in the world.”
                </p>
              </div>

              <h2 id="teaching-machines">Teaching machines at all</h2>
              <p>
                The lecture walked through how we even got to the point of “teaching” machines: from early programs that followed rules to systems that learn from data and feedback. What stood out was how much of that history is about us deciding what to optimize for. Rule-based systems encoded human judgment directly; statistical and neural approaches learn from data that itself reflects human choices—what to label, what to reward, what to include or exclude.
              </p>

              <ul class="key-terms">
                <li><span class="term">Post-training</span> The phase after pretraining where we shape behavior via reward, feedback, and evaluation.</li>
                <li><span class="term">Alignment</span> The problem of making model objectives match intended (human) goals and values.</li>
                <li><span class="term">Reward / objective</span> What we optimize for; often a proxy for what we actually want.</li>
              </ul>

              <h2 id="where-it-stands">Where the project stands</h2>
              <p>
                Right now we’re in a phase where models are powerful but brittle, helpful but sometimes harmful, and often bad at knowing their own limits. Post-training—reward models, alignment, evaluations—is the work of closing that gap. It’s not just technical; it’s about being explicit about what we want, what we’re willing to trade off, and how we hold both the systems and ourselves accountable.
              </p>

              <p>
                The lifecycle from pretraining to deployment involves many levers: data, objectives, feedback, red-teaming, monitoring. Each step embeds assumptions about “good” behavior. Making those assumptions visible and contestable is part of the work—and part of what we might owe the people affected by these systems.
              </p>

              <h2 id="what-we-owe">What we owe</h2>
              <p>
                I’m still wrestling with the question of what we actually owe machines. Do we owe them “good” objectives? Transparency about how we shape them? Or is the obligation mostly to the people affected by what those machines do? This week was a good reminder that the answer isn’t in the code alone—it’s in how we frame the problem and who we include in that conversation.
              </p>

              <p>
                The lecture didn’t resolve that question, and I don’t think it was meant to. It set the stage: we’re not just building smarter tools; we’re building systems that will make consequential decisions in ambiguous situations. How we train them, what we optimize for, and how we evaluate and deploy them are all sites where “what we owe” gets decided—in practice, if not in theory.
              </p>

              <h2 id="what-clicked">What clicked for me</h2>
              <p>
                The idea that “we choose what counts as success” hit different after the lecture. I’d been thinking about ML as mostly a technical problem: better data, better objectives, better scaling. But the history of teaching machines—from rule-based systems to learned reward—is really a history of us deciding what to optimize for. Every reward function, every preference dataset, every safety filter is a choice. That choice is where the ethics live, and it’s not something we can outsource to the model.
              </p>
              <p>
                I also liked the framing of “problems without right answers.” In a lot of my other classes we optimize for something well-defined (accuracy, loss, etc.). Here we’re dealing with things like “be helpful but not sycophantic,” “be honest about uncertainty,” “don’t cause harm across diverse contexts”—none of which map cleanly to a single number. That’s the part I want to get better at: designing objectives and evaluations for exactly those fuzzy cases.
              </p>

              <h2 id="sitting-with">One thing I'm sitting with</h2>
              <p>
                The question I keep coming back to: is the obligation to the machines (give them “good” objectives, be transparent about how we shape them) or to the people affected by what those machines do? I’m leaning toward the latter—I think we owe people clarity, accountability, and a say in how these systems behave—but I’m not sure we can fully separate the two. If we treat the system as a black box and only worry about outcomes, we might miss ways the design itself is unjust. If we focus only on “aligning” the model to some notion of good, we might miss who gets to define that notion. So I’m holding both in mind and seeing how the rest of the course sharpens it.
              </p>
              <p>
                Really glad to be in this class. On to the lifecycle of a language model next week.
              </p>
            </div>

            <footer class="article-footer">
              <p class="toc-title">Tags</p>
              <div class="tags">
                <span class="tag">alignment</span>
                <span class="tag">ethics</span>
                <span class="tag">post-training</span>
                <span class="tag">objectives</span>
                <span class="tag">Module 1</span>
              </div>
              <a href="#main" class="back-to-top">↑ Back to top</a>
            </footer>
          </div>
        </article>
      </div>
    </main>

    <footer class="site-footer">
      <div class="wrap">
        <span class="footer-mono">CDSS 94 · Spring 2026 · Building Thoughtful AI Systems</span>
      </div>
    </footer>
  </body>
</html>
